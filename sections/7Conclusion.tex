\section{Conclusion}\label{sec:conclusion}

The analysis in this paper illustrates how machine learning systems can be seen as an agency relationship between a selector and actor. This moves away from the standard approach of modeling ``learning agents" as monolithic entities that take actions to try to maximize their reward. There is a tension between this abstraction and the fact that in reality, selectors choose algorithms, not actions, to maximize their expected reward. Rather than abstracting away from this by considering it as an approximation error to an optimal choice of policy, we explicitly model the selector's choice as an algorithm. We can then frame problems of machine learning in terms of the bounded rationality and information constraints that induce an agency cost. This approach can abstract away from technical details, while still modeling the essential characteristic of machine learning, and focus on general considerations about the interaction between selector and actor. 

This paper has introduced an initial model of the actor and selector as boundedly rational agents in a two-stage game, and a resulting mis-alignment between their utility functions. The central mechanism is as follows: The first effect is due to the actor's bounded rationality: The actor has to take decisions in an environment, and the selector has an interest in choosing the actor's utility function so as to maximize its own expected utility. But because the actor has limited understanding of the causal structure of its environment, the selector is incentivized to integrate its information about that causal structure into the utility function of the actor. As a result, the actor does not pursue the same goals as the selector, but pursues ``proxy goals". The main formal result of this paper gives conditions under which the selector simply ``projects" its expected utility onto the proxy variables whose causal structure the actor understands. 

The second effect is due to the selector's bounded rationality and imperfect information, and coincides closely with the training data of the selector: The selector has a limited domain of signals for which it has knowledge (informally, the training data). Beyond that domain, we assume that the selector has no information. The result is that the selector optimizes for a limited amount of causal structures, which causes it to give proxy goals that are specifically tailored to the environments over which the selector has information. Moreover, it causes the set of optimal utility functions that it could give the actor to not be uniquely aligned with its own, even if the actor has perfect information. This is simply a reframing of ``limited training data" in the context of this model. Both of these effects result in the creation of an AI agent that pursues goals that are different from those specified by the human programmers. This model is thus a particular formalization of the inner alignment problem \citep{Hubinger2019}.

This paper has thus introduced an initial model of learning algorithms and AI agents in a principal-agent relation. There is potential for a broader agency theoretic analysis of machine learning systems, and this paper can thus be seen as an initial step in that direction. Agency relations are a central phenomenon in human societies, and have been studied extensively by economists. But as artificially intelligent agents become more advanced, and are given an increasingly influential role in society and the economy, agency relations between AI systems may reach the level of importance of agency relations between humans. %I therefore propose that computer scientists and economists work on this problem. 


