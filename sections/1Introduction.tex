\pagebreak
\section{Introduction}

The field of artificial intelligence (AI) has made significant progress in recent years in developing autonomous \textit{intelligent agents} that perform tasks that until recently were beyond the capability of machines: artificially intelligent agents (or AI agents) are now able to defeat world-class professional players of complex games such as Go \citep{Silver2016,Silver2017} and StarCraft II \citep{alphastarblog2019}; They have started to approach human-level competence at complex skills such as object recognition \citep{He2015}, and the physical control and movement of humanoid robots \citep{Andrychowicz2019}, autonomous drones \citep{Floreano2015}, and autonomous vehicles, or ``self-driving cars" \citep{Schwarting2018}. As AI agents become more advanced, they will most likely be given a larger role in society, such as economic and military affairs \citep{bostrom2014, Russell2015, Schwab2016}. It therefore becomes increasingly important to understand how AI agents will behave, in order to ensure that their role in society is as beneficial as possible \citep{Rahwan2019}. 

This paper introduces and analyzes a particular kind of principal-agent model \textit{between two AI systems}. We argue that this problem is inherent in the current paradigm of artificial intelligence (called \textit{machine learning}), and that this model can provide some understanding of the behavior of AI agents. This model, and the questions we want to ask about it, are quite different from those in the context of human principal-agent relations: Some constraints and considerations that apply to humans simply do not apply to AI agents, and vice versa. Nevertheless, the fundamental source of the agency problem is the same: AI agents have \textit{imperfect information}, and \textit{bounded rationality} (bounded computational capabilities).

There will be two decision makers, both of which are \textit{computing devices}: an ``actor" (the agent), and a ``selector" (the principal). They could be software programs, or physical computing devices. We will use the example of self-driving cars as an illustration.

The \textit{actor} is an AI agent. It is a piece of intelligent software that is going to take actions in the world. In the case of self-driving cars, it is the software that actually controls the car. It is physically located inside the car (e.g. behind the dashboard), and will receive some \textit{signals}, such as images from the cameras on its front and rear, a measured speed signal from its wheels, a gas meter, a location signal from a GPS system, and so forth; And it will decide on certain \textit{actions}, such as setting the angle of the steering wheel, how hard to activate the gas, and whether to break. 

It is in the interest of human users that the actor behaves in a certain way. For example, software that controls a self-driving car (the actor) should make sure that the car does not crash into another car, does not hit pedestrians, follows the traffic rules, and reach the specified destination. However, humans are not able to manually write the algorithms that are able to do such complex behavior. The approach that is currently taken in the \textit{machine learning} paradigm of AI, is that rather than writing a program manually, the human specifies an \textit{objective function}, or \textit{utility function}. This utility function will not say what \textit{actions} the car should take, but what \textit{outcomes} of its actions would be good or bad according to humans (do not hit pedestrians, reach the destination, and so forth). But the human will not itself optimize this utility function: The human writes another program (distinct from the actor), which is going to do this. This program is called the \textit{learning algorithm},\footnote{More accurately, in the terminology of \cite{Russell2010} it is the combination between a learning algorithm or \textit{learning element}, together with the reward element.} but we will call it the \textit{selector}, and the utility function that the human gave to it $U_S$.

The \textit{selector} is the principal in our model, and like the actor it is a piece of software. However, unlike the actor, it is not itself going to take actions, but is going to select the \textit{algorithm} that the actor will execute to determine its actions. In the case of self-driving cars, the selector is typically not located in the car itself, but on a \textit{remote server}. It does not itself observe the camera images that the actor observes as the actor is driving around, and cannot send messages in real-time to the actor to influence its behavior. Rather, it is going to specify some (potentially quite complex) set of rules that the actor will execute in the form of an algorithm. It is going to choose this algorithm in order to maximize the utility function $U_S$ that the human gave it. A (particularly bad) rule could be: no matter the visual input data, always drive straight ahead. 

We can in principle consider different types of algorithms that the selector could select. However, in this paper we are going to make the theoretical assumption that the selector cannot specify the actions that the actor will take directly, but \textit{delegates} decision making to the actor. It does this by specifying in the actor's algorithm a utility function that the actor should maximize. \footnote{The reasoning behind assuming that the actor is a utility maximizer is discussed in section \ref{sec:existingwork:ML}. }




\begin{figure}[H]
	\centering
	\captionsetup{labelfont=bf,font=small,labelsep=period}
	\includegraphics[width=0.8\linewidth]{"images/conceptual-diagrams/selector-actor-car"}
	\caption{\rightskip=20pt\leftskip=20pt In the example of a self-driving car, the actor is a computer device in the car, and the selector is a program running on a \textit{remote server} of the car company. The selector does not influence the decision making of the actor in \textit{real-time}: It only selects an \textit{algorithm}, which the actor will then execute autonomously to make its decisions. We will model this as a principal-agent problem, where the algorithm plays the role of the contract.
	}
	\label{fig:selector-actor-car} 	
\end{figure}


\medskip \noindent So the setup is as follows: The selector and actor are both utility maximizers. The selector's utility function $U_S$ is exogenously specified by the humans (and captures the desired outcomes: following the traffic rules, avoid killing pedestrians, reach the specified destination, and so forth). But the actor's utility function $U_A$ is \textit{not exogenous}: It will be chosen by the selector. Given the $U_A$ that the selector chose, the actor will then actually take actions to maximize its expected $U_A$, based on observations of signals from the environment. The selector is going to predict how the actor would behave depending on the utility function $U_A$ that the selector would give it, and will choose $U_A$ such that the actor would take actions that avoids killing pedestrians, and so forth.

It is natural to ask: Why would the selector not just give the actor the utility function $U_S$, so that the actor always makes the right decisions? The key point of this paper, is that due to the \textit{bounded rationality} of the actor and selector, this might be a sub-optimal choice, or not uniquely optimal. The actor has limited computational abilities and a limited ability to understand its environment. In the self-driving car example, the actor might have a limited ability to model how deciding to drive very fast might result in hitting a pedestrian. It is unable to understand this causal relation. The selector on the other hand, has a limited ability to reason about data it has not seen. In the self-driving car example, the remote server cannot in real-time observe the actor's signals, and if it has not yet seen that data in the past, it is computationally infeasible to reason about it in advance.

We will model this bounded rationality as a limited knowledge about what outcomes will result from the actor's actions. This is similar to \citeauthor{Spiegler2016}'s economic analysis of bounded rationality modeled as distorted subjective causal model \citeyearpar{Spiegler2016,Spiegler2019}, and is in line with the use of causal models in computer science to describe the environments of agents \citep{Everitt2019}. We can model the effects of the actor's actions as a \textit{causal graph} \citep{Pearl2000,Dawid2002}, which captures the idea that the actor's actions have direct effects on certain variables (speed of the car), but also indirect effects (whether the car hits a pedestrian). The bounded rationality is modeled by assuming for example, that the actor does not know that by speeding the car up, it will hit a pedestrian. As an illustration see figure \ref{fig:intro-self-driving-car-bounded-rationality}. The selector gives the actor a utility function that says ``Don't hit yellow shapes". This causes the actor to avoid hitting the pedestrian, because they were wearing yellow clothes, but the actor is doing it for a different reason than the humans intended.

\begin{figure}[h]
	\centering
	\captionsetup{labelfont=bf,font=small,labelsep=period}
	\includegraphics[width=0.7\linewidth]{"images/intro-images/intro-self-driving-car-bounded-rationality"}
	\caption{\rightskip=20pt\leftskip=20pt Bounded rationality of the actor is a source of \textit{misalignment} between selector ($U_S$) and actor ($U_A$): The actor has a set of actions (a square): It can either step on the gas, or brake. This will cause the car to either speed up or stop. But now assume that there is a pedestrian wearing yellow clothes in front of the car. If the car speeds up, it will hit and kill the pedestrian. But assume now that the actor does not understand this, because the actor only perceives a \textit{yellow shape}, and does not know that what it is seeing is a pedestrian. This effect on the pedestrian is a dotted line: there is an effect, but the actor does not know it.		
	In this situation, the selector is better off giving the actor the goal ``do not hit yellow shapes", rather than ``do not hit pedestrians". This will cause the actor to slow down, to avoid hitting a yellow shape, which in this case coincides with what the selector wants, which is to avoid hitting pedestrians.
	}
	\label{fig:intro-self-driving-car-bounded-rationality} 	
\end{figure}

But why is this a problem? The actor ends up not killing the yellow-clothed pedestrian. But imagine that now, the actor encounters a pedestrian that is wearing \textit{green} clothes. The actor will drive the car straight into the pedestrian, since it only cares about avoiding yellow shapes, not pedestrians in general. This was the result of the actor's bounded understanding of the situation, and of the selector's failure to predict the occurrence of a green-clothed pedestrian, and to give the actor a better algorithm. 

The result is that the actor receives a utility function that happens to work conditional on a specific expectation about the actor's environment, rather than a utility function that encodes the actual goals that humans have. The selector gave the actor a sub-optimal algorithm as a result of its imperfect prediction, and to compensate for the actor's bounded rationality.

These problems are well-known in the context of machine learning, and the problem of the selector's failure to choose an algorithm that works for a broader set of situations than those that it has data about, is called the problem of ``robustness" \citep{Amodei2016}. In this paper, I will model this problem as an agency problem.

The central concern of this paper is: How can humans ensure that the actors is endowed with the utility function that the humans specified to the selector? In our earlier work, we have called this \textit{the inner alignment problem} \citep{Hubinger2019}. To this end, I will set up a model of a two-stage game between a boundedly rational selector and a boundedly rational actor, where the selector endows the actor with a utility function encoded in an algorithm. The paper is concerned with two primary questions: Firstly, what factors determine which utility function $U_A$ that the selector gives to the actor, and will it be misaligned with its own utility $U_S$? Secondly, when is it uniquely optimal for the selector to give the actor a $U_A$ that is equivalent to its own $U_S$, and what prevents this uniqueness?

\medskip

\noindent The paper proceeds as follows: section \ref{sec:existingwork} will present the main idea informally. In particular, we discuss the standard model of AI agents that choose actions to maximize their reward function, and argues in more detail for instead considering the AI system as consisting of a principal (selector) and agent (actor). Section \ref{sec:model} states the model. Section \ref{sec:causal:examples} outlines a number of simple toy examples to illustrate the idea. Section \ref{sec:mainresultproxy} addresses question $1$ given restrictive assumptions by formalizing the mechanism described above, and section \ref{sec:revealedpreference} addresses question $2$. Section \ref{sec:conclusion} concludes.
